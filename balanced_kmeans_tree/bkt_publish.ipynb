{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如何用PySpark实现高性能的分布式Balanced K-means tree¶\n",
    "\n",
    "Balanced K-means tree是一种均衡划分数据集的算法，树的每一个叶节点代表一个簇，簇内数据的相似度大于簇间数据的相似度。与一般K-means不同的是，Balanced K-means tree对每个簇中的数据量（即簇大小）做出了限制，要求每个簇大小不能小于某个用户设定的最小值，并且不能大于该最小值的2倍。此外用户对树的高度也可以做出限制。 通过限定每个簇的大小，Balanced K-means tree很适合用于K近邻搜索算法的空间划分，比如微软最近开源的\n",
    "ANN库（approximate nearest neighborhood search）就用到了Balanced K-means tree做索引的空间划分。 \n",
    "\n",
    "Balanced K-means tree的形状如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](BKT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Redis cluster来存储K-means每个簇的数据\n",
    "\n",
    "Balanced K-means tree会使用K-means算法反复迭代聚类产出新的簇，在每一轮K-means算法完成后，它形成的簇需要存储在合适的地方供下次迭代使用，另外在最终的树构建完成后检索时也需要使用每个簇中的数据。我们\n",
    "暂时选择了redis来存储每个簇的数据。\n",
    "\n",
    "由于我们的数据集是numpy array类型的，为了高性能的读取，适合使用redis的string类型来存储，其中string的key由tree id + parent node id + cluster id构成，value就是numpy array转换成的字节序列。 此外需要注意的是，由于redis string value的最大值是512MB，如果numpy array大小超过了512MB，则需要将numpy array分片存储。为了代码的高内聚和低耦合，我们可以将使用redis cluster的代码封装在一个RedisDBWrapper类中，在该类初始化的时候连接redis cluster得到一个可复用的连接池,得到连接池句柄。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "class RedisDBWrapper(object):\n",
    "\n",
    "    #Redis string数据类型的value最大值是500MB\n",
    "    MAX_CHUNK_SIZE = 500000000\n",
    "    #在存储数据前，先要计算每个chunk的大小，公式为 RedisDBWrapper.CHUNK_SIZE = int(RedisDBWrapper.MAX_CHUNK_SIZE / data.shape[1])\n",
    "    CHUNK_SIZE = 100000\n",
    "\n",
    "    def __init__(self, host, port = 6379):\n",
    "        pool = redis.ConnectionPool(host = host, port = port, decode_responses = True)\n",
    "        self.redis = redis.Redis(connection_pool = pool)\n",
    "        \n",
    "    def getHandler(self):\n",
    "        return self.redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RedisDBWrapper类用于完成读取和存储numpy array数据，对外暴露的接口有2个，其中一个是save_data，另一个是get_data。 save_data用来将指定Key的numpy array数据存储到redis cluster中，它一开始会根据传入的numpy array数据的大小来判断需要分成几个chunk（或分片）来存储，接着在存储head chunk和其余的chunk时会有所区别，因为在第一个chunk中除了存储分片本身的维度外，还需要存储整个numpy array的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#由于redis string value的大小限制，我们需要将一个很大的numpy array拆分成一个个chunk来存储\n",
    "def __save_chunk_data(self, data, key, index):\n",
    "    \"\"\"Store given Numpy array 'a' in Redis under key 'n'\"\"\"\n",
    "    try:\n",
    "        #每一个numpy array字节序列分片的维度存储在chunk的首部\n",
    "        h, w = data.shape\n",
    "        shape = struct.pack('>II', h, w)\n",
    "        encoded = shape + data.tobytes()\n",
    "\n",
    "        self.redis.set(key + \"_\" + str(index), encoded.decode('latin1'))\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "\n",
    "#numpy array第一个数据分片除了存储本分片的维度之外，还需要存储整个numpy array的维度\n",
    "def __save_head_chunk_data(self, data, key, total_w, total_h, chunk_num):\n",
    "\n",
    "    try:\n",
    "        h, w = data.shape\n",
    "        head = struct.pack('>III', total_w, total_h, chunk_num)\n",
    "        shape = struct.pack('>II', h, w)\n",
    "        encoded = head + shape + data.tobytes()\n",
    "\n",
    "        self.redis.set(key + \"_0\", encoded.decode('latin1'))\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "   \n",
    "\n",
    "#将numpy array存储在redis中\n",
    "def save_data(self, data, key):\n",
    "\n",
    "    size = 0\n",
    "    index = 0\n",
    "\n",
    "    if len(data) <= RedisDBWrapper.CHUNK_SIZE:\n",
    "        chunk_num = 1\n",
    "    elif len(data) % RedisDBWrapper.CHUNK_SIZE == 0:\n",
    "        chunk_num = int(len(data) / RedisDBWrapper.CHUNK_SIZE)\n",
    "    else:\n",
    "        chunk_num = int(len(data) / RedisDBWrapper.CHUNK_SIZE) + 1\n",
    "\n",
    "    while size < len(data):\n",
    "\n",
    "        offset = min(RedisDBWrapper.CHUNK_SIZE, len(data) - size)\n",
    "\n",
    "        firstChunk = False\n",
    "        if size == 0:\n",
    "            firstChunk = True\n",
    "\n",
    "        if index == 0:\n",
    "            ret = self.__save_head_chunk_data(data[size : size + offset], key, data.shape[0], data.shape[1], chunk_num)\n",
    "        else:\n",
    "            ret = self.__save_chunk_data(data[size : size + offset], key, index)\n",
    "\n",
    "        if not ret:\n",
    "            return False\n",
    "\n",
    "        size += offset\n",
    "\n",
    "        index += 1\n",
    "\n",
    "    return True\n",
    "\n",
    "RedisDBWrapper.__save_chunk_data = __save_chunk_data\n",
    "RedisDBWrapper.__save_head_chunk_data = __save_head_chunk_data\n",
    "RedisDBWrapper.save_data = save_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_data则负责将所有chunk从redis cluster读取出来并做完数据拼接之后，能够重新reshape将原来的数据维度恢复出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取numpy array的其中一个数据分片\n",
    "def __get_chunk_data(self, key, index):\n",
    "    \"\"\"Retrieve Numpy array from Redis key 'n'\"\"\"\n",
    "    try:\n",
    "        encoded = self.redis.get(key + \"_\" + str(index))\n",
    "        if not encoded:\n",
    "            return None\n",
    "        encoded = encoded.encode('latin1')\n",
    "\n",
    "        h, w = struct.unpack('>II', encoded[:8])\n",
    "        return np.frombuffer(encoded, offset = 8).reshape(h,w)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "#读取numpy array的第一个分片数据，需要将整个numpy array的维度都读取出来\n",
    "def __get_head_chunk_data(self, key):\n",
    "\n",
    "    try:\n",
    "        encoded = self.redis.get(key + \"_0\")\n",
    "        if not encoded:\n",
    "            return 0, 0, 0, None\n",
    "        encoded = encoded.encode('latin1')\n",
    "\n",
    "        total_h, total_w, chunk_num = struct.unpack('>III', encoded[:12])\n",
    "        h, w = struct.unpack('>II', encoded[12 : 20])\n",
    "        return total_h, total_w, chunk_num, np.frombuffer(encoded, offset = 20).reshape(h,w)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0, 0, 0, None\n",
    "\n",
    "\n",
    "#从redis中读取出numpy array\n",
    "def get_data(self, key):\n",
    "\n",
    "    total_h, total_w, chunk_num, head_chunk_data = self.__get_head_chunk_data(key)\n",
    "    if total_h == 0:\n",
    "        return None\n",
    "\n",
    "    for index in range(chunk_num - 1):\n",
    "        chunk_data = self.__get_chunk_data(key, index + 1)\n",
    "\n",
    "        head_chunk_data = np.concatenate((head_chunk_data, chunk_data), axis = 0)\n",
    "\n",
    "    head_chunk_data.reshape(total_h, total_w)\n",
    "    return head_chunk_data\n",
    "\n",
    "RedisDBWrapper.__get_chunk_data = __get_chunk_data\n",
    "RedisDBWrapper.__get_head_chunk_data = __get_head_chunk_data\n",
    "RedisDBWrapper.get_data = get_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分布式K-means算法的pyspark定制实现\n",
    "\n",
    "K-means算法是构成Balanced k-means tree的基础，它的性能决定了构建树的性能。以下代码是我们实现的高性能的分布式K-means算法，目前只是实现了最传统的K-means算法，以后还会加上优化初始点分布的K-means++算法，因为初始点的选择会影响K-means算法的收敛速度。\n",
    "\n",
    "首先我们可以利用spark的分布式运算特性将数据集分区，每个数据分区可以在不同的spark executor中并行处理，这样就大大加快了整体运算速度。k-means算法需要反复迭代来求出各个新的cluster的中心点，如果不使用分布式的做法，我们直接可以计算所有数据到上次迭代所形成的各个cluster的中心距离，为每一个样本点选出一个最近的中心，从而将样本点划归到最近的cluster中去，然后再重新计算新cluster的中心。但是随着数据集规模的增长，这样全量计算的速度会变得越来越慢。如果使用spark的分布式运算特性，我们就可以将数据分区，分别计算每个分区中的样本点到上次迭代所形成的各个cluster的中心距离， 划归到最近的cluster中，并返回每个数据分区中的各个cluster的数据总和以及样本量。下面FastKmeans类中的assign_to_new_centroids就是在每个spark RDD分区中计算各个cluster的数据总和以及样本量。\n",
    "\n",
    "其次在spark driver中收集到所有spark RDD分区中的各个cluster的数据总和及样本量之后，就可以归并计算各个cluster的完整数据总和及完整样本量，从而快速计算出新cluster的中心。FastKmeans类中的move_to_new_centroids就是用来执行这个任务的。\n",
    "\n",
    "K-means算法就这样反复迭代运行，直到中心点不再发生变化为止。在这里我们根据上次迭代的cluster中心和本次计算的cluster中心的距离来判断K-means算法是否收敛，当它们之间的距离小于0.1时即认为收敛。最后我们需要把K-means算法产生的各个cluster的数据存储在redis中，方便以后读取利用。在这里我们还是利用spark的分布式运算特性来将数据集分区，将各个分区中的数据划分到各自所属的cluster，然后按cluster id聚合起来，从而将每一个cluster的数据并行地存储在redis中。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class FastKmeans(object):\n",
    "\n",
    "    iterations = 1000\n",
    "    converge_threshold = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means算法的初始中心点选择很重要，主要有两方面影响，一是算法的收敛速度，二是初始中心选择不好会算法会产生空的簇，如下图所示：\n",
    "\n",
    "![](empty_cluster.png)\n",
    "\n",
    "图中假设有6个点需要聚成3个类，红蓝绿分别代表所选的初始节点，由于蓝色点和红色点的距离太小，到第2轮迭代时，蓝点所在的簇就成了空簇。\n",
    "\n",
    "针对这种情况，有人提出了K-means++算法，所谓K-means++，就是改进了K-means的初始中心点选择，原始K-means是从样本中按照均匀分布来选择初始中心点，而K-means++则尽量地让初始中心点彼此远离。具体步骤如下：\n",
    "1. 随机选择一个样本点作为初始中心集的第一个中心\n",
    "2. 计算每个样本点离初始中心集的最近距离\n",
    "3. 按照一定概率选择一个样本点放入初始中心集，这个概率和样本点离初始中心集的最近距离正相关\n",
    "4. 重复步骤2-3，直到初始中心集构建完毕（即选择完了K个中心点）\n",
    "\n",
    "通过一定的概率让初始中心点彼此远离后，就可以提高收敛速度，并且产生空簇的可能性也大大降低。\n",
    "\n",
    "以下就是按照这个算法来选择初始中心点的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def get_closest_distance(iterator, centroids):\n",
    "    centroids = centroids.value\n",
    "\n",
    "    data = np.array([x for x in iterator])\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    points = np.array([x[0] for x in data])\n",
    "    points_idx = np.array([x[1] for x in data])\n",
    "\n",
    "    if len(points) == 0:\n",
    "        return []\n",
    "\n",
    "    distances = np.sqrt(((points - centroids[:, np.newaxis]) ** 2).sum(axis = 2))\n",
    "    min_distances  = np.min(distances, axis = 0)\n",
    "\n",
    "    for i, idx in enumerate(points_idx):\n",
    "        yield (idx, min_distances[i])\n",
    "\n",
    "\n",
    "@staticmethod\n",
    "def initialize_centroids_non_uniform(points, k, sc):\n",
    "    n_centroids = np.zeros(shape = (k, points.shape[1]))\n",
    "\n",
    "    centroid = points[random.sample(range(points.shape[0]), 1)]\n",
    "    n_centroids[0] = centroid\n",
    "\n",
    "    rdd = sc.parallelize(points).zipWithIndex().cache()\n",
    "\n",
    "    for i in range(k - 1):\n",
    "\n",
    "        bc_centroids = sc.broadcast(n_centroids)\n",
    "\n",
    "        data_closest_distances_rdd = rdd.mapPartitions(lambda x: FastKmeans.get_closest_distance(x, bc_centroids)).cache()\n",
    "\n",
    "        sum_all = data_closest_distances_rdd.map(lambda x: (1, x[1])).reduceByKey(lambda x, y: x + y).collect()[0][1]\n",
    "\n",
    "        sum_all *= np.random.random()\n",
    "\n",
    "        distances = 0\n",
    "\n",
    "        data_closest_distances = data_closest_distances_rdd.collect()\n",
    "        for p in data_closest_distances:\n",
    "            distances += p[1] \n",
    "\n",
    "            if distances >= sum_all:                              \n",
    "                n_centroids[i + 1] = points[p[0]]\n",
    "                break\n",
    "\n",
    "    return n_centroids\n",
    "\n",
    "FastKmeans.get_closest_distance = get_closest_distance\n",
    "\n",
    "FastKmeans.initialize_centroids_non_uniform = get_closest_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成初始中心选择后，就开始聚类，其中assign_to_new_centroids这个类静态函数会计算每个RDD分区中的数据应该分配到哪个新cluster中，返回新的cluster id，cluster中的数据总和以及cluster的大小, 这里计算出的新cluster的信息（比如数据总和和cluster大小）只是由这个RDD分区中数据所建立起来的，可以看作是cluster的局部信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def assign_to_new_centroids(iterator, centroids):\n",
    "    centroids = centroids.value\n",
    "    points = np.array([x for x in iterator])\n",
    "    if len(points) == 0:\n",
    "        return [(k, np.zeros(shape = (1, centroids.shape[1])), 0) for k in range(centroids.shape[0])]\n",
    "\n",
    "    distances = np.sqrt(((points - centroids[:, np.newaxis]) ** 2).sum(axis = 2))\n",
    "    closest =  np.argmin(distances, axis = 0)\n",
    "\n",
    "    sum = np.array([points[closest == k].sum(axis = 0) for k in range(centroids.shape[0])])\n",
    "    count = np.array([len(points[closest == k]) for k in range(centroids.shape[0])])\n",
    "\n",
    "    for k in range(centroids.shape[0]):\n",
    "        yield (k, sum[k], count[k])\n",
    "        \n",
    "FastKmeans.assign_to_new_centroids = assign_to_new_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "move_to_new_centroids这个类静态函数会合并所有RDD分区计算出的cluster局部信息, 返回新的cluster id和cluster的完整大小。注意，如果出现空簇就马上返回，此轮聚类作废，需要再次重新选择初始中心点进行新的一轮聚类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def move_to_new_centroids(collect_data):\n",
    "\n",
    "    centors_map = {}\n",
    "    for cid, sum, size in collect_data:\n",
    "        if cid not in centors_map:\n",
    "            centors_map[cid] = ([sum], [size])\n",
    "        else:\n",
    "            centors_map[cid][0].append(sum)\n",
    "            centors_map[cid][1].append(size)\n",
    "\n",
    "    centroids = []\n",
    "    cluster_sizes = []\n",
    "\n",
    "    for k, v in centors_map.items():\n",
    "        if np.array(v[1]).sum() == 0:\n",
    "            return None, None\n",
    "        \n",
    "        centroids.append(np.array(v[0]).sum(axis = 0) / np.array(v[1]).sum())\n",
    "        cluster_sizes.append(np.array(v[1]).sum())\n",
    "\n",
    "    return np.array(centroids), np.array(cluster_sizes)\n",
    "\n",
    "FastKmeans.move_to_new_centroids = move_to_new_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize_centroids这个类静态函数初始化cluster中心，这里暂时使用随机初始化，以后会使用K-means++中的算法来选择最优的cluster中心，使得各个cluster中心的距离尽可能大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def initialize_centroids(points, k):\n",
    "    \"\"\"returns k centroids from the initial points\"\"\"\n",
    "    centers_id = random.sample(range(points.shape[0]), k)\n",
    "    return points[centers_id]\n",
    "\n",
    "FastKmeans.initialize_centroids = initialize_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在K-means聚类完成后，就可以将各个cluster的数据存储到redis集群中，assign_to_final_centroids会将一个RDD分区中的数据和它们所属的cluster id关联起来，而save_cluster_to_redis就负责将一个cluster的数据存储在redis中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def assign_to_final_centroids(iterator, centroids):\n",
    "    points = np.array([x for x in iterator])\n",
    "    if len(points) == 0:\n",
    "        return [(k, []) for k in range(centroids.shape[0])]\n",
    "\n",
    "    distances = np.sqrt(((points - centroids[:, np.newaxis]) ** 2).sum(axis = 2))\n",
    "    closest =  np.argmin(distances, axis = 0)\n",
    "\n",
    "    for k in range(centroids.shape[0]):\n",
    "        yield (k, points[closest == k])\n",
    "\n",
    "@staticmethod\n",
    "def save_cluster_to_redis(cid, iterator, shape_w, redis_host, redis_port, cluster_key):\n",
    "    points = np.zeros(shape = (1, shape_w))\n",
    "    for x in iterator:\n",
    "        points = np.concatenate((points, x), axis = 0)\n",
    "\n",
    "    r = RedisDBWrapper(host = redis_host, port = redis_port)\n",
    "\n",
    "    ret = r.save_data(points[1:], cluster_key + '_' + str(cid))\n",
    "\n",
    "    return (cid, points.sum(axis = 0) / (len(points) - 1), len(points) - 1, ret)\n",
    "\n",
    "FastKmeans.assign_to_final_centroids = assign_to_final_centroids\n",
    "FastKmeans.save_cluster_to_redis = save_cluster_to_redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit这个类静态函数是FastKmeans的用户调用接口，驱动以上的函数采用迭代的方式来完成K-means聚类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def center_diff(a, b):\n",
    "    d = a - b\n",
    "\n",
    "    return (d * d).sum()\n",
    "\n",
    "#FastKmeans的调用接口\n",
    "#参数 data:        numpy array数组\n",
    "#参数 K:           划分成的cluster数量\n",
    "#参数 redis_host:  用于数据存储的redis host\n",
    "#参数 redis_port:  用于数据存储的redis port\n",
    "#参数 sc:          spark context\n",
    "#参数 cluster_key: 用来标识此次聚类的唯一key\n",
    "@staticmethod\n",
    "def fit(data, k, redis_host = None, redis_port = 6379, sc = None, cluster_key = None):\n",
    "\n",
    "    if not cluster_key:\n",
    "        cluster_key = str(int(time.mktime(time.localtime())))\n",
    "\n",
    "    retris = 0\n",
    "\n",
    "    while True:\n",
    "        n_centors = FastKmeans.initialize_centroids_non_uniform(data, k, sc)\n",
    "        n_centors = n_centors[n_centors[:, 0].argsort()]\n",
    "\n",
    "        centors = n_centors\n",
    "\n",
    "        rdd = sc.parallelize(data)\n",
    "\n",
    "        for i in range(FastKmeans.__iterations):\n",
    "            #print(\"round %d ...\"%i)\n",
    "\n",
    "            broadcast_centors = sc.broadcast(n_centors)\n",
    "\n",
    "            collect_data = rdd.mapPartitions(lambda x: FastKmeans.assign_to_new_centroids(x, broadcast_centors)).collect()\n",
    "\n",
    "            n_centors, cluster_sizes = FastKmeans.move_to_new_centroids(collect_data)\n",
    "            if n_centors is None:\n",
    "                break\n",
    "\n",
    "            n_centors = n_centors[n_centors[:, 0].argsort()]\n",
    "\n",
    "            if FastKmeans.center_diff(n_centors, centors) < FastKmeans.__converge_threshold:\n",
    "                break\n",
    "\n",
    "            centors = n_centors\n",
    "\n",
    "        if n_centors is not None:\n",
    "            break\n",
    "\n",
    "        retris += 1\n",
    "        if retris >= FastKmeans.__MAX_RETRY_TIMES:    \n",
    "            return None\n",
    "\n",
    "    #收敛后就把数据存储到redis cluster中\n",
    "    if redis_host:\n",
    "        result = rdd.mapPartitions(lambda x: FastKmeans.assign_to_final_centroids(x, n_centors)).groupByKey().\\\n",
    "                     map(lambda x: FastKmeans.save_cluster_to_redis(x[0], x[1], n_centors.shape[1], redis_host, redis_port, cluster_key)).collect()\n",
    "    else:\n",
    "        result = n_centors\n",
    "\n",
    "    return result\n",
    "\n",
    "FastKmeans.center_diff = center_diff\n",
    "FastKmeans.fit = fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分布式K-means算法的pyspark定制实现的性能\n",
    "\n",
    "我们测试算法的benchmark 来自于texmex corpus的ANN测试数据集，地址在http://corpus-texmex.irisa.fr/。\n",
    "\n",
    "这是一个非常古老的数据集，这些向量样本是从一些图像上面抽取出来的SIFT特征点。以下代码用于读取该数据集文件并在内存中转为numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fvecs_read(filename, c_contiguous=True):\n",
    "    fv = np.fromfile(filename, dtype=np.float32)\n",
    "    if fv.size == 0:\n",
    "        return np.zeros((0, 0))\n",
    "    dim = fv.view(np.int32)[0]\n",
    "    assert dim > 0\n",
    "    fv = fv.reshape(-1, 1 + dim)\n",
    "    if not all(fv.view(np.int32)[:, 0] == dim):\n",
    "        raise IOError(\"Non-uniform vector sizes in \" + filename)\n",
    "    fv = fv[:, 1:]\n",
    "    if c_contiguous:\n",
    "        fv = fv.copy()\n",
    "    return fv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 128)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fvecs_read('sift/sift_learn.fvecs').astype(np.float64)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下表是我们自己定制的K-means实现和numpy库以及spark mlib中的K-means实现的性能对比：\n",
    "\n",
    "![](kmeans_perf.png)\n",
    "\n",
    "可见我们的算法实现性能优于numpy库和spark mlib的k-means实现，尤其比spark mlib的k-means实现快2倍。numpy库的k-means实现性能接近于我们的定制实现，但它是单机版的，如果数据量过大就不适用了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建Balanced K-means tree\n",
    "\n",
    "Balanced K-means tree是一棵多叉树，使用K-means算法反复迭代聚类产出新的簇。它的规则如下：\n",
    "\n",
    "1. 如果一个簇比用户设定的最小值的2倍还要大，则继续划分这个簇，划分的数量为 min(size / max_cluster_size + 1, max_clusters_per_run), 其中size为该簇大小，max_cluster_size为用户指定的最小簇大小的2倍， max_clusters_per_run为用户设定的每次聚类的最大划分数（即用户设定的每次聚类产生的最大簇数量）\n",
    "\n",
    "2. 如果根据公式min(size / max_cluster_size + 1, max_clusters_per_run)计算出来的划分数量为2，则将该簇分成两个大小相等的子簇\n",
    "\n",
    "3. 如果一个簇大于用户设定的最小值并且小于该最小值的2倍，则该簇对应树的一个叶子节点\n",
    "\n",
    "4. 如果在一次聚类产生的簇中，有多个簇小于用户设定的最小值，则把这些小簇做归并，直到归并后的簇大于用户设定的最小值\n",
    "\n",
    "此外，该Balanced K-means tree的实现还支持使用redis来保存树和加载树，这样在完成一次树的训练构造后，如果不用时就可以将树从内存中卸载，等到下次再使用时从redis中加载。\n",
    "\n",
    "要实现树这种数据结构，按惯用做法，我们先定义树的节点，树节点的成员有表征该节点的唯一id号，表示该节点所在簇的cluster id，簇中心centroid以及它的父节点和子节点列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import random\n",
    "import queue\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "#代表Balanced K-means tree的节点\n",
    "class BKTNode(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.id = str(id(self))\n",
    "        self.cluster_id = None\n",
    "        self.children = []\n",
    "        self.parent = None\n",
    "        self.centriod = None\n",
    "        self.leaf = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_data是树节点的成员函数，用于读取该节点的数据，如果是叶节点，则只读取它所对应的簇数据，如果是非叶节点，则递归读取它子树的所有簇数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#参数 redis_handler:  redis handler\n",
    "#参数 bkt_key:        该节点所在的树的唯一标识\n",
    "def get_data(self, redis_handler, bkt_key):\n",
    "    if not redis_handler:\n",
    "        return None\n",
    "\n",
    "    if self.leaf:\n",
    "        return redis_handler.get_data(bkt_key + '_' + self.parent.id + '_' + str(self.cluster_id))\n",
    "\n",
    "    merged_data = np.zeros(shape = (1, self.centriod.shape[0]))\n",
    "\n",
    "    for child in self.children:\n",
    "        data = child.get_data(redis_handler, bkt_key)\n",
    "        merged_data = np.concatenate((merged_data, data), axis = 0)\n",
    "\n",
    "    return merged_data[1:]\n",
    "\n",
    "BKTNode.get_data = get_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们看看BKT(Balance K-means tree的缩写）的实现，按照常规做法，树需要有一个根节点root_node，唯一标识该树的bkt_key，此外我们还需要指定树的结构参数，比如树的最大高度max_depth，簇的最小元素数量min_cluster_size, 簇的最大元素数量max_cluster_size, 如果不指定簇的最大元素数量，则默认为最小数量的2倍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#代表Balanced K-means tree\n",
    "class BKTree(object):\n",
    "    pass\n",
    "\n",
    "#初始化Balanced K-means tree\n",
    "#参数 max_clusters_per_run: 指定每次K-means聚类的最大簇数量\n",
    "#参数 max_depth           : 指定树的高度限制\n",
    "#参数 min_cluster_size    : 指定簇中数据的最小数量，即最小簇大小\n",
    "#参数 sc                  : spark context\n",
    "#参数 redis_host          : 指定用于存储簇数据的redis host\n",
    "#参数 redis_port          : 指定用于存储簇数据的redis port\n",
    "#参数 max_cluster_size    : 指定簇中数据的最大数量，是个软约束，如果不指定的话就默认是min_cluster_size的2倍\n",
    "def init(self, max_clusters_per_run, max_depth, min_cluster_size, sc, redis_host, redis_port = 6379, max_cluster_size = 0, balance = True):\n",
    "    self.__max_clusters_per_run = max_clusters_per_run\n",
    "    self.__max_depth = max_depth\n",
    "    self.__min_cluster_size = min_cluster_size\n",
    "    self.__max_cluster_size = max(self.__min_cluster_size, max_cluster_size)\n",
    "    self.__sc = sc\n",
    "    self.__redis_host = redis_host\n",
    "    self.__redis_port = redis_port\n",
    "    self.__bkt_key = str(id(self))\n",
    "    self.__balance = balance\n",
    "    self.__root_node = BKTNode()\n",
    "\n",
    "    self.__redis = RedisDBWrapper(redis_host, redis_port)\n",
    "\n",
    "def get_root(self):\n",
    "    return self.__root_node\n",
    "\n",
    "def get_key(self):\n",
    "    return self.__bkt_key\n",
    "\n",
    "def get_redis(self):\n",
    "    return self.__redis.getHandler()\n",
    "\n",
    "BKTree.init = init\n",
    "BKTree.get_root = get_root\n",
    "BKTree.get_key = get_key\n",
    "BKTree.get_redis = get_redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dump是树的成员函数，该函数的功能就是把树的完整结构存储在redis中，我们使用树的广度优先遍历算法将每一个树节点的信息保存下来，包括它的id，所代表的簇id和子节点id。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump(self):\n",
    "\n",
    "    epochTime = int(time.mktime(time.localtime()))\n",
    "\n",
    "    tree_params = self.__bkt_key + '_' + str(self.__max_clusters_per_run) + '_' + str(self.__max_depth) + '_' \\\n",
    "                  + str(self.__min_cluster_size) + '_' + str(self.__max_cluster_size) + '_' \\\n",
    "                  + str(self.__balance) + '_' + self.__root_node.id\n",
    "\n",
    "    self.get_redis().sadd('my_bkt', str(epochTime) + '_' + tree_params)\n",
    "\n",
    "    q = queue.Queue()\n",
    "    q.put(self.__root_node)\n",
    "\n",
    "    try:\n",
    "        while not q.empty():\n",
    "            node = q.get()\n",
    "\n",
    "            if node.leaf:\n",
    "                value = '1_' + str(node.cluster_id)\n",
    "            elif node.cluster_id:\n",
    "                value = '0_' + str(node.cluster_id)\n",
    "            else:\n",
    "                value = '0_0'\n",
    "\n",
    "            ret = self.get_redis().set(self.__bkt_key + '_' + node.id, value)\n",
    "            if not ret:\n",
    "                raise Exception(\"Failed to dump BKT node to redis\")\n",
    "\n",
    "            for child in node.children:\n",
    "                ret = self.get_redis().sadd(self.__bkt_key + '_' + node.id + '_children', child.id)\n",
    "                if not ret:\n",
    "                    raise Exception(\"Failed to dump BKT sub tree to redis\")\n",
    "\n",
    "                q.put(child)\n",
    "\n",
    "        return True \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    \n",
    "BKTree.dump = dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既然dump是负责将树存储在redis中，那么必然有将树从red|is中重新加载进内存的方法，loads就是这样的函数。loads也使用广度优先遍历算法来重新构建树，然后调用update_centroid来重新计算每个非叶节点的簇中心。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_centroid(self, node):\n",
    "\n",
    "    if node.leaf:\n",
    "        data = self.__redis.get_data(bkt.__bkt_key + '_' + node.parent.id + '_' + str(node.cluster_id))\n",
    "        total_sum = data.sum(axis = 0)\n",
    "        total_size = len(data)\n",
    "\n",
    "        node.centriod = total_sum / total_size\n",
    "\n",
    "        return (total_sum, total_size)\n",
    "\n",
    "    cluster_list = []\n",
    "    for child in node.children:\n",
    "        cluster_list.append(self.update_centroid(child))\n",
    "\n",
    "    total_sum = np.zeros(shape = (cluster_list[0][0].shape[0], ))\n",
    "    total_size = 0\n",
    "\n",
    "    for sum, size in cluster_list:\n",
    "        total_sum += sum\n",
    "        total_size += size\n",
    "\n",
    "    node.centriod = total_sum / total_size\n",
    "\n",
    "    return (total_sum, total_size)\n",
    "\n",
    "\n",
    "@classmethod\n",
    "def loads(cls, bkt_key, redis_host, redis_port = 6379):\n",
    "\n",
    "    redis = RedisDBWrapper(redis_host, redis_port)\n",
    "\n",
    "    trees = redis.getHandler().smembers('my_bkt')\n",
    "    found = False\n",
    "\n",
    "    for tree in trees:            \n",
    "        if bkt_key == tree.split('_')[1]:\n",
    "\n",
    "            _, bkt_key, max_clusters_per_run, \\\n",
    "            max_depth, min_cluster_size, max_cluster_size, balance, root_id = tree.split('_')\n",
    "\n",
    "            found = True\n",
    "            break\n",
    "\n",
    "    if not found:\n",
    "        return None\n",
    "\n",
    "    bkt = BKTree(int(max_clusters_per_run), int(max_depth), int(min_cluster_size), \\\n",
    "                 None, redis_host, redis_port, int(max_cluster_size), bool(balance))\n",
    "\n",
    "    bkt.__bkt_key = bkt_key\n",
    "    bkt.__redis = redis\n",
    "    bkt.__root_node.id = root_id\n",
    "\n",
    "    try:\n",
    "        q = queue.Queue()\n",
    "        q.put(bkt.__root_node)\n",
    "\n",
    "        while not q.empty():\n",
    "            node = q.get()\n",
    "\n",
    "            if not node.leaf:\n",
    "                children_ids = bkt.get_redis().smembers(bkt.__bkt_key + '_' + node.id + '_children')\n",
    "            else:\n",
    "                data = bkt.__redis.get_data(bkt.__bkt_key + '_' + node.parent.id + '_' + str(node.cluster_id))\n",
    "\n",
    "                node.centriod = data.sum(axis = 0) / len(data)\n",
    "                children_ids = []\n",
    "\n",
    "\n",
    "            for child_id in children_ids:\n",
    "                child_info = bkt.get_redis().get(bkt.__bkt_key + '_' + child_id)\n",
    "\n",
    "                child_node = BKTNode()\n",
    "                child_node.id = child_id\n",
    "\n",
    "                child_node.leaf = True if child_info.split('_')[0] == '1' else False\n",
    "                child_node.cluster_id = int(child_info.split('_')[1])\n",
    "                child_node.parent = node\n",
    "\n",
    "                q.put(child_node)\n",
    "\n",
    "                node.children.append(child_node)\n",
    "\n",
    "        bkt.update_centroid(bkt.__root_node)\n",
    "\n",
    "        return bkt\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "BKTree.update_centroid = update_centroid\n",
    "BKTree.loads = loads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建Balance K-means tree的目的是为了快速检索最相似的样本，给定一个样本，我们需要搜索出与该样本相似度最高的一个簇，再在这个簇里面检索与该样本最相似的K个样本。在这里我们逐个计算样本与树节点所代表的簇的簇中心的距离，选择最近的树节点并沿着该节点的子树继续上述查找过程，直到找到距离最近的叶节点，返回最近叶节点中的数据。类方法get_nearest_leaf_node用于查找与样本最接近的叶节点，get_nearest_cluster用于读取并返回最近叶节点中的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_nearest_cluster(self, point, node, leaf_clusters):\n",
    "\n",
    "    if node.leaf:\n",
    "        leaf_clusters.append((node, np.sqrt(((point - node.centriod) ** 2).sum())))\n",
    "\n",
    "    for child in node.children:\n",
    "        self.__get_nearest_cluster(point, child, leaf_clusters)\n",
    "\n",
    "#返回与指定的数据点最接近的簇所对应的节点\n",
    "#参数 point: 要检索的数据点\n",
    "def get_nearest_leaf_node(self, point):\n",
    "\n",
    "    leaf_clusters = []\n",
    "\n",
    "    for child in self.__root_node.children:\n",
    "        self.__get_nearest_cluster(point, child, leaf_clusters)\n",
    "\n",
    "    if len(leaf_clusters) == 0:\n",
    "        return None\n",
    "\n",
    "    leaf_clusters = sorted(leaf_clusters, key = lambda x: x[1])\n",
    "\n",
    "    return leaf_clusters[0][0]\n",
    "\n",
    "#获取与指定的数据点最接近的簇数据\n",
    "#参数 point: 要检索的数据点\n",
    "def get_nearest_cluster(self, point):\n",
    "\n",
    "    node = self.get_nearest_leaf_node(point)\n",
    "    if node:\n",
    "        return node.get_data(self.__redis, self.__bkt_key)\n",
    "\n",
    "    return None\n",
    "\n",
    "BKTree.__get_nearest_cluster = __get_nearest_cluster\n",
    "BKTree.get_nearest_leaf_node = get_nearest_leaf_node\n",
    "BKTree.get_nearest_cluster = get_nearest_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建Balance K-means tree主要由build和make_bkt_node方法来完成。build方法是构建树的驱动函数，它首先采用K-means算法将整个数据集分成k个大簇，然后调用make_bkt_node方法来构建子树，最后调用adjust_tree_depth方法来调整树的高度。make_bkt_node是一个重要方法，它负责树节点的构造，如果该节点中的数据量不小于min_cluster_size并且不大于max_cluster_size，则是一个叶节点，否则继续使用K-means算法来划分节点数据递归地构建它的子树。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建balanced k-means tree\n",
    "#参数 data:  numpy array类型的数据集 \n",
    "def build(self, data):\n",
    "    size = len(data)\n",
    "    if size == 0:\n",
    "        return self.__root_node\n",
    "    \n",
    "    max_cluster_size = max(self.__min_cluster_size * 2, self.__max_cluster_size)\n",
    "\n",
    "    if size <= max_cluster_size:\n",
    "        cur_node = BKTNode()\n",
    "        cur_node.parent = self.__root_node\n",
    "        cur_node.centriod = data.sum(axis = 0) / len(data)\n",
    "        cur_node.cluster_id = 0\n",
    "\n",
    "        self.__redis.save_data(data, self.__bkt_key + '_' + self.__root_node.id + \"_\" + str(cur_node.cluster_id))\n",
    "        self.__root_node.children.append(cur_node)\n",
    "\n",
    "        return self.__root_node\n",
    "\n",
    "    k = min(size / max_cluster_size + 1, self.__max_clusters_per_run)\n",
    "\n",
    "    clusters = FastKmeans.fit(data, k, self.__redis_host, self.__redis_port, self.__sc, self.__bkt_key + '_' + self.__root_node.id)\n",
    "\n",
    "    for cid, centriod, _, ret in clusters:\n",
    "        if not ret:\n",
    "            print(\"Failed to build BKT due to redis write error\")\n",
    "            return None\n",
    "\n",
    "        child_node = self.make_bkt_node(cid, centriod, self.__root_node)\n",
    "        if not child_node:\n",
    "            print(\"Failed to build BKT\")\n",
    "            return None\n",
    "\n",
    "        self.__root_node.children.append(child_node)\n",
    "\n",
    "    self.adjust_tree_depth()\n",
    "\n",
    "    return self.__root_node\n",
    "\n",
    "#构建balanced k-means tree的节点\n",
    "#参数 centriod_id： 该节点所对应的簇id\n",
    "#参数 centriod：    该节点所对应的簇中心\n",
    "#参数 parent_node： 该节点的父节点\n",
    "def make_bkt_node(self, cluster_id, centriod, parent_node):\n",
    "\n",
    "    cur_node = BKTNode()\n",
    "    cur_node.parent = parent_node\n",
    "    cur_node.centriod = centriod\n",
    "    cur_node.cluster_id = cluster_id\n",
    "\n",
    "    #读取该节点所对应簇的数据\n",
    "    data = self.__redis.get_data(self.__bkt_key + '_' + parent_node.id + '_' + str(cluster_id))\n",
    "\n",
    "    size = len(data)\n",
    "    max_cluster_size = max(self.__min_cluster_size * 2, self.__max_cluster_size)\n",
    "\n",
    "    #如果该节点所对应簇的数据量小于max_cluster_size，则完成一个叶子节点\n",
    "    if size <= max_cluster_size:\n",
    "        cur_node.leaf = True\n",
    "        return cur_node\n",
    "\n",
    "    #计算该节点中的数据还可以分成几个簇，如果只能分成2个簇，则做均分处理，否则进行一轮K-means聚类\n",
    "    k = min(int(size / max_cluster_size + 1), self.__max_clusters_per_run)\n",
    "    if self.__balance:\n",
    "        if k == 2:\n",
    "            clusters = self.half_cut_cluster(data, self.__bkt_key + '_' + cur_node.id)\n",
    "        else:\n",
    "            clusters = FastKmeans.fit(data, k, self.__redis_host, self.__redis_port, self.__sc, self.__bkt_key + '_' + cur_node.id)\n",
    "    else:\n",
    "        clusters = FastKmeans.fit(data, k, self.__redis_host, self.__redis_port, self.__sc, self.__bkt_key + '_' + cur_node.id)\n",
    "\n",
    "    if False in [ret for _, _, _, ret in clusters]:\n",
    "        print(\"Failed to build BKT due to redis write error\")\n",
    "        return None\n",
    "\n",
    "    #检查K-means聚类所产生的簇，将小于min_cluster_size的簇归并起来\n",
    "    if k > 2 and self.__balance:\n",
    "        clusters = self.merge_small_clusters(clusters, cur_node)\n",
    "\n",
    "    #对聚类所形成的新簇递归构建子树\n",
    "    for cid, centriod, _, _ in clusters:\n",
    "        child_node = self.make_bkt_node(cid, centriod, cur_node)\n",
    "        if not child_node:\n",
    "            return None\n",
    "\n",
    "        cur_node.children.append(child_node)\n",
    "\n",
    "    return cur_node\n",
    "\n",
    "BKTree.build = build\n",
    "BKTree.make_bkt_node = make_bkt_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在构建树的过程中，需要用到2个辅助方法，一个是half_cut_cluster，另一个是merge_small_clusters。\n",
    "\n",
    "half_cut_cluster负责将一个簇切成大小相等的2个小簇，如果一个簇的数据量只够切成2个小簇，则使用对半切，因为我们对簇的最小数据量有硬性的需求，如果不对半切而继续使用k-means来分成2个小簇的话，可能会造成一个小簇的数据量永远少于规定的最小数据量，从而算法无法收敛。\n",
    "\n",
    "merge_small_clusters负责处理K-means聚类产生的小簇，如果多个小簇的数据量都小于规定的最小数据量，则将这些小簇合并起来直到合并后的簇的数据量不小于min_cluster_size并且小大于max_cluster_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将数据切分成大小相等的两个簇\n",
    "def half_cut_cluster(self, data, cluster_key, cluster_ids = ()):\n",
    "\n",
    "    clusters = []\n",
    "    if len(cluster_ids) == 0:\n",
    "        first_id = 0\n",
    "        second_id = 1\n",
    "    else:\n",
    "        first_id, second_id = cluster_ids\n",
    "\n",
    "    data_len = int(len(data) / 2)\n",
    "\n",
    "    ret = self.__redis.save_data(data[:data_len], cluster_key + \"_\" + str(first_id))\n",
    "    cluster = (first_id, data[:data_len].sum(axis = 0) / data_len, data_len, ret)\n",
    "    clusters.append(cluster)\n",
    "\n",
    "    data_len = len(data) - int(len(data) / 2)\n",
    "    ret = self.__redis.save_data(data[int(len(data) / 2):], cluster_key + \"_\" + str(second_id))\n",
    "    cluster = (second_id, data[int(len(data) / 2):].sum(axis = 0) / data_len, data_len, ret)\n",
    "    clusters.append(cluster)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "#检查K-means聚类所产生的簇，将小于min_cluster_size的簇归并起来\n",
    "def merge_small_clusters(self, clusters, parent_node):\n",
    "\n",
    "    clusters = sorted(clusters, key = lambda x: x[2])\n",
    "    to_merges = []\n",
    "\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        if cluster[2] < self.__min_cluster_size:\n",
    "            to_merges.append((cluster[0], cluster[2]))\n",
    "        else:\n",
    "            clusters = clusters[i:]\n",
    "            break\n",
    "\n",
    "    max_cluster_size = max(self.__min_cluster_size * 2, self.__max_cluster_size)\n",
    "\n",
    "    if len(to_merges) == 0:\n",
    "        return clusters\n",
    "\n",
    "    if len(to_merges) == len(clusters):\n",
    "        clusters = []\n",
    "        new_cid = max([cluster[0] for cluster in to_merges]) + 1\n",
    "    else:\n",
    "        new_cid = max([cluster[0] for cluster in clusters]) + 1\n",
    "\n",
    "    i = 0\n",
    "    new_clusters = []\n",
    "\n",
    "    while i < len(to_merges):\n",
    "\n",
    "        merged_data = np.zeros(shape = (1, parent_node.centriod.shape[0]))\n",
    "\n",
    "        size = 0\n",
    "\n",
    "        while i < len(to_merges) and size + to_merges[i][1] <= max_cluster_size:\n",
    "\n",
    "            data = self.__redis.get_data(self.__bkt_key + '_' + parent_node.id + '_' + str(to_merges[i][0]))\n",
    "\n",
    "            merged_data = np.concatenate((merged_data, data), axis = 0)\n",
    "\n",
    "            size += to_merges[i][1]\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        if size >= self.__min_cluster_size and size <= max_cluster_size:\n",
    "\n",
    "            merged_data = merged_data[1:]\n",
    "            ret = self.__redis.save_data(merged_data, self.__bkt_key + '_' + parent_node.id + '_' + str(new_cid))\n",
    "\n",
    "            new_cluster = (new_cid, merged_data.sum(axis = 0) / len(merged_data), len(merged_data), ret)\n",
    "            new_clusters.append(new_cluster)\n",
    "\n",
    "        #如果就剩下最后一个小簇，那么表示在它前面的小簇都合并完了，并且合并后的大小小于max_cluster_size，那么就把这最后一个小簇也合并掉，\n",
    "        #这样这个簇大小肯定超过max_cluster_size了，可以继续在下次迭代中分裂。\n",
    "        if i == len(to_merges) - 1:\n",
    "\n",
    "            data = self.__redis.get_data(self.__bkt_key + '_' + parent_node.id + '_' + str(to_merges[i][0]))\n",
    "\n",
    "            merged_data = np.concatenate((merged_data, data), axis = 0)\n",
    "            ret = self.__redis.save_data(merged_data, \\\n",
    "                                         self.__bkt_key + '_' + parent_node.id + '_' + str(new_cid))\n",
    "\n",
    "            new_cluster = (new_cid, merged_data.sum(axis = 0) / len(merged_data), len(merged_data), ret)\n",
    "            new_clusters[len(new_clusters) - 1] = new_cluster\n",
    "            break\n",
    "\n",
    "        #如果小簇合并后还是小于min_cluster_size，那么就和第一个大簇合并，并且将合并之后的簇再对半切成两个均等的簇，继续在下次迭代中分裂。\n",
    "        #在这一步中，如果不做对半切，算法可能永远无法收敛\n",
    "        if size < self.__min_cluster_size and len(clusters) > 0:\n",
    "\n",
    "            data = self.__redis.get_data(self.__bkt_key + '_' + parent_node.id + '_' + str(clusters[0][0]))\n",
    "            merged_data = np.concatenate((merged_data, data), axis = 0)\n",
    "\n",
    "            new_clusters.extend(self.half_cut_cluster(merged_data[1:], self.__bkt_key + '_' + parent_node.id, (new_cid, new_cid + 1)))\n",
    "            new_cid += 1\n",
    "            clusters = clusters[1:]\n",
    "            break\n",
    "\n",
    "        new_cid += 1\n",
    "\n",
    "    new_clusters.extend(clusters)\n",
    "\n",
    "    return new_clusters\n",
    "\n",
    "BKTree.half_cut_cluster = half_cut_cluster\n",
    "BKTree.merge_small_clusters = merge_small_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在构建完树结构之后，我们可能还要根据指定的最大树高度来调整树的高度。get_tree_depth用于返回树的高度，采用了深度优先遍历的算法。adjust_tree_depth负责调整树的高度，它会判断当前的树高度，如果大于指定的最大高度，则调用__adjust_tree_depth来调整。__adjust_tree_depth方法会采用递归的深度优先遍历算法，先找到子节点全是叶节点的非叶节点，将它的所有叶节点都挂载到它的父节点上，再将该节点裁剪掉，然后再层层往上调整非叶节点的簇中心点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取指定节点的子树高度\n",
    "def get_tree_depth(self, node):\n",
    "\n",
    "    if len(node.children) == 0:\n",
    "        return 0\n",
    "\n",
    "    depths = []\n",
    "    for child in node.children:\n",
    "        depths.append(self.get_tree_depth(child) + 1)\n",
    "\n",
    "    return max(depths)\n",
    "\n",
    "#调整指定节点下的子树高度\n",
    "def __adjust_tree_depth(self, node):\n",
    "\n",
    "    if len(node.children) == 0:\n",
    "        return\n",
    "\n",
    "    all_leaf_childs = np.array([child.leaf for child in node.children]).all()\n",
    "\n",
    "    if not all_leaf_childs:\n",
    "        children = node.children\n",
    "\n",
    "        for child in children:\n",
    "            if not child.leaf:\n",
    "                self.__adjust_tree_depth(child)\n",
    "\n",
    "        if len(children) != len(node.children):\n",
    "            merged_data = np.zeros(shape = (1, node.centriod.shape[0]))\n",
    "\n",
    "            for child in node.children:\n",
    "                data = self.__redis.get_data(self.__bkt_key + '_' + node.id + '_' + str(child.cluster_id))\n",
    "\n",
    "                merged_data = np.concatenate((merged_data, data), axis = 0)\n",
    "\n",
    "            node.centriod = merged_data.sum(axis = 0) / (len(merged_data) - 1)\n",
    "\n",
    "        return\n",
    "\n",
    "    parent_cluster_ids = [child.cluster_id for child in node.parent.children]\n",
    "    new_cluster_id = max(parent_cluster_ids) + 1\n",
    "\n",
    "    for child in node.children:\n",
    "        data = self.__redis.get_data(self.__bkt_key + '_' + node.id + '_' + str(child.cluster_id))\n",
    "        if data is None:\n",
    "            print(self.__bkt_key + '_' + node.id + '_' + str(child.cluster_id))\n",
    "\n",
    "        self.__redis.save_data(data, self.__bkt_key + '_' + node.parent.id + '_' + str(new_cluster_id))\n",
    "\n",
    "        child.cluster_id = new_cluster_id\n",
    "        child.parent = node.parent\n",
    "\n",
    "        node.parent.children.append(child)\n",
    "\n",
    "        new_cluster_id += 1\n",
    "\n",
    "    node.parent.children.remove(node)\n",
    "\n",
    "#调整树高度\n",
    "def adjust_tree_depth(self):\n",
    "\n",
    "    while self.get_tree_depth(self.__root_node) > self.__max_depth:\n",
    "        self.__adjust_tree_depth(self.__root_node)\n",
    "        \n",
    "BKTree.get_tree_depth = get_tree_depth\n",
    "BKTree.__adjust_tree_depth = __adjust_tree_depth\n",
    "BKTree.adjust_tree_depth = adjust_tree_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试Balanced K-means tree\n",
    "\n",
    "在构建完Balanced K-means tree后，我们就可以开始着手测试。测试算法的benchmark同样来自于texmex corpus的ANN测试数据集，测试步骤如下：\n",
    "\n",
    "启动spark session，这个magic函数是我们为机器学习平台打造的，指定driver和executor的内存各为10GB，在每个work主机上使用一个executor，每个executor进程使用4个CPU核。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#启动spark session\n",
    "%start_pyspark --driver_memory 10g --executor_memory 10g --executor_instances 1 --executor_cores 4 --driver_maxResultSize 10g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据，这批数据一共有十万个特征点，SIFT的维度是128。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 128)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fvecs_read('sift/sift_learn.fvecs').astype(np.float64)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为我们使用redis来存储数据，而redis的string类型对value长度有最大限制，超过限制就要将数据分片存储，所以需要计算一个分片中所包含的最大数据个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取合适的redis chunk size\n",
    "RedisDBWrapper.CHUNK_SIZE = int(RedisDBWrapper.MAX_CHUNK_SIZE / data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化Balanced K-means tree, 指定最大聚类簇数量为5，树最大高度为4， 最小簇大小为2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkt = BKTree()\n",
    "bkt.init(max_clusters_per_run = 5, max_depth = 4, min_cluster_size = 2500, sc = sc, redis_host = \"10.10.50.32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于数据集data来构建Balanced K-means tree，并计算构建时间，可见最终100000个数据花了1分17秒就完成了树的构建，性能还是蛮快的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.5 s, sys: 1.12 s, total: 21.7 s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%time root = bkt.build(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们看看树的检索，给定data[59909]这个样本点，从树中搜索出和它最接近的簇，并查看下该簇中数据的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3391, 128)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取数据data[59909]的最近邻簇\n",
    "nearest_cluster = bkt.get_nearest_cluster(data[59909])\n",
    "nearest_cluster.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们将树从内存中dump到redis集群，再重新加载进内存，看看前后树是否一致："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bkt.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkt_reload = BKTree.loads(bkt.get_key(), \"10.10.50.32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3391, 128)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_cluster2 = bkt_reload.get_nearest_cluster(data[59909])\n",
    "nearest_cluster2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(nearest_cluster == nearest_cluster2).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
